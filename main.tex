\documentclass[a4paper, 12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{float}
\usepackage{placeins}
\usepackage{xstring}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{color}
\usepackage{subcaption}
\usepackage[hidelinks]{hyperref}
\usepackage{cleveref}


\geometry{tmargin=2.5cm, bmargin=3cm}

% Geen geïndenteerde paragrafen, da's maar lelijk.
\setlength{\parindent}{0pt}
% Wel spatie tussen paragrafen please.
\setlength{\parskip}{1em}

% Exponential notation in '\num{}'w/ Ex instead of *10^x
\sisetup{output-exponent-marker=\ensuremath{\mathrm{E}}}

% Necessary to make space->underscore substitution work. 
\usepackage[T1]{fontenc}
% Command that typesets variables in fixed width font, and with spaces automatically replaced by underscores.
\newcommand{\var}[1]{\texttt{\StrSubstitute{#1}{ }{\_\allowbreak}}}

% For identifying individual datapoints -- i.e. cars.
\newcommand{\car}[3]{``#1 \textit{#2} #3''}

\newcommand{\wide}[1]{\makebox[\textwidth][c]{#1}%
}

% For figures accross the page -- wider than the text width
\newcommand{\widefig}[1]{\wide{\includegraphics[width=0.9\paperwidth]{#1}}}

\newcommand{\widedoublefig}[2]{\wide{
  \centering
  \begin{subfigure}[b]{0.65\textwidth}
    \includegraphics[width=\textwidth]{#1}
  \end{subfigure}
  \begin{subfigure}[b]{0.65\textwidth}
    \includegraphics[width=\textwidth]{#2}
  \end{subfigure}
  }
}

\newcommand{\sectionref}[1]{section ``\nameref{#1}''}

\newcommand{\n}[1]{\num{#1}}

\begin{document}


\input{titlepage}


\section{Linear regression}

The complete dataset of 1500 cars was split into a 1000-car training set and a 500-car test set. We continue this analysis with the training set.

\subsection*{Explorative analysis \& outlier removal}
\label{outlier_removal}

We search for outliers by calculating, for each data point, its Mahalanobis distance -- using robust estimates for the center and the covariance matrix. Specifically, the Minimum Covariance Determinant estimator \cite{mcd} was used, approximated by the \texttt{Fast MCD} algorithm \cite{fastmcd}. \Cref{outliers} shows the resulting robust distances.

\begin{figure}%[h]
  \widefig{img/multivar_outlier_all}
  \vspace{1mm} \newline
  \widefig{img/multivar_outlier_zoom}
  \caption{Robust distance-to-center of each car in the training set.}
  \label{outliers}
\end{figure}

Three strong outliers are found:
\begin{itemize}[topsep=0pt,itemsep=0pt]
  \item the \car{Volkswagen}{Jetta (from NOV 06 Wk 45 >)}{1.4 TSI (170 PS) Sport} with an abnormal \var{noise level} of 0.3;
  \item the \car{Vauxhall}{Signum MY2008}{3.0CDTi V6 24v with 16/17/18" wheel} with an abnormal \var{nox emissions} value of 237000;
  \item the \car{MG Rover Group}{Streetwise}{1.8} without any individually abnormal variables.
\end{itemize}
Note that this third outlier would not have been found had we only looked at univariate (or even bivariate) distributions of the data.

The found strong outliers are removed from the training set.


\begin{figure}%[H]
  \widefig{img/pairs.pdf}
  \caption{Uni- and bivariate sample distributions (after the strongest outliers have been removed, as described in \sectionref{outlier_removal}).}
  \label{pairs}
\end{figure}

\Cref{pairs} shows the uni- and bivariate distributions in the data set. Note the very strong correlations between \var{urban metric}, \var{extra urban metric}, \var{combined metric}, and \var{co2} -- and to a lesser extend \var{engine capacity}. This is in accordance with their meanings: cars with larger engine volumes consume more liters of fuel (all three \var{ metrics} measure fuel consumption), and every liter of fuel corresponds to a fixed amount of $\mathrm{CO_2}$.

We also note that for the variable \var{noise level}, a large subset of cars take on discrete values. (There are also cars with \var{noise level}s in between). This discrete character would negatively impact a cluster analysis, as the clusters would tend to form around the disrete values; while other variables would have disproportionately less impact on the clustering. However, we decide this will probably not impact the subsequent regression and classification tasks too much, so we keep this variable, for now.


\subsection*{Model construction}

We will construct a general linear model to predict the \var{co2} variable. The predictor variables to include in the model will be selected via bidirectional stepwise regression. The Akaike Information Criterion (AIC) is used to compare models. The least squares estimator will always be used to estimate the coefficients $\boldsymbol{\beta}$ for a candidate model.

\paragraph{Predictor variable transformations}
\label{par:var_transform}\leavevmode\\
The AIC assumes the residuals to be normally distributed. As they are an affine transformation of the predictor variables, the predictor variables need to be normally distributed as well. Thus, before starting the stepwise variable selection procedure, we find a Box-Cox power transformation for all continuous predictor variables that maximises their normality in the maximum likelihood sense. The thusly found exponents $\lambda$ are listed in \cref{table:predictorvar_transform}, along with the $p$-values for the Shapiro-Wilk test of normality before and after the transformation. Note that, except for \var{noise level}, the normality of each variable greatly increases after the transformation. This was confirmed using normal QQ plots of the variables. We apply the maximum likelihood transformation to all continuous predictor variables, except for \var{noise level} (as its maximum likelihood exponent falls outside the usual range, and its normality does not significantly improve after transformation).

\begin{table}
\centering
\input{table_lambdas}
\caption{Maximum likelihood transformation exponents $\lambda$, and $p$-values for the Shapiro-Wilk test of normality before and after transformation, for each possible continuous predictor variable, as described in \sectionref{par:var_transform}.}
\label{table:predictorvar_transform}
\end{table}

\paragraph{Response variable transformation}
\label{par:responsevar_transfrom}\leavevmode\\
We now create a first linear model (via stepwise AIC regression starting from a model containing all predictor variables). \Cref{pre-bc-response} shows the residuals of the resulting model. The strong (nonlinear) correlation between the predicted \var{co2} values and the residuals prompts us to transform the response variable. We find the optimal Box-Cox power transformation of the response variable for $\lambda = 0.30303$ (see \cref{fig:lambda_ml}). We round this to $0.3$ and create a new linear model with the transformed response variable.

\begin{figure}
  \widedoublefig
  {img/pre-bc-response_pre-removal_fitted-stdres}
  {img/pre-bc-response_pre-removal_qq}
  \caption{Standardised residuals of the linear model before the response variable has been transformed. Fitted values versus standardised residuals, and normal QQ plot of the standardised residuals.}
  \label{pre-bc-response}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.7\linewidth]{img/pre-bc-response_pre-removal_lambda-ml}
  \caption{Log-likelihood of $\lambda$ for the Box-Cox transformation of the response variable of the linear model described in \sectionref{par:responsevar_transfrom}.}
  \label{fig:lambda_ml}
\end{figure}

\paragraph{Removal of residual outliers}\leavevmode\\
The residuals of the model with transformed response variable are shown in \cref{post-bc-response_pre-removal}. We find three strong vertical outliers. They are:
\begin{itemize}[topsep=0pt,itemsep=0pt]
  \item the \car{Volkswagen}{LT 35 Kombi MWB - LWB}{2.8 (158 PS) TDI Axle Ratio 3.727};
  \item the \car{Honda}{Insight, Model Year 2012}{1.3 IMA HS, HS-T, HX};
  \item and the \car{Renault}{New M\'egane Hatchback}{2.0 dCi 160 FAP}.
\end{itemize}
(These cars do not have higher robust distances than other cars (see \cref{outliers}); they can however be spotted as outliers when the Mahalonobis distance is considered). These three additional outliers are removed from the training set. We note that removing these outliers has no effect on the maximum-likelihood $\lambda$ of the Box-Cox transformation of the response variable.

\begin{figure}
  \widedoublefig
  {img/post-bc-response_pre-removal_fitted-stdres}
  {img/post-bc-response_pre-removal_qq}
  \caption{Standardised residuals of the linear model after the response variable has been transformed, but before vertical outliers have been removed. Fitted values versus standardised residuals, and normal QQ plot of the standardised residuals.}
  \label{post-bc-response_pre-removal}
\end{figure}

\paragraph{stepaic finally}\leavevmode\\
- finally
- two seeds, they converge
- refer to table
- (maybe mention: rather high p nox)
- (R2, R2adj)
- ANOVA
- vars not included
- (final AIC)

The procedure will be started both from a model containing all predictor variables, and from an empty model containing only a constant term.

convergence!

\begin{figure}
  \widedoublefig
  {img/post-bc-response_post-removal_fitted-stdres}
  {img/post-bc-response_post-removal_qq}
  \caption{Standardised residuals of the final linear model. Fitted values versus standardised residuals, and normal QQ plot of the standardised residuals.}
  \label{post-bc-response_post-removal}
\end{figure}

\begin{table}
\centering
\wide{\input{table_lm}}
\caption{Coefficients and ANOVA of the final general linear model.}
\end{table}


\subsection*{Model assumptions}

For a given model (that is, for a given selection of predictor variables and for given tranformations of the predictor variables and/or the response variable), for the OLS estimator to be the best linear unbiased estimator of the model parameters, the Gauss-Markov conditions need to be satisfied. The first condition, that the errors have expectation zero, is satisfied by construction. The other conditions -- that the errors have equal variance and are uncorrelated -- cannot be proven, but we can check whether some necessary conditions seem to be satisfied.
(residual plots)

\subsection*{Multicollinearity}

extra:
- interaction terms
- nonlinear regression: ANN, SVM (dataset quite large)
- check on test set

\section{Classification}
\subsection*{Variable selection}
\subsection*{Logistic model}
\subsection*{Interpretation}
\subsection*{Apparent error rate}
\subsection*{Error rate on test set}
\subsection*{Linear discriminant analysis}
\subsection*{Quadratic discriminant analysis}
\subsection*{Comparison}


\begin{thebibliography}{9}
\bibitem{mcd} 
Rousseeuw, P. J. and Leroy, A. M. 
\textit{Robust Regression and Outlier Detection}. 
Wiley, 1987.

\bibitem{fastmcd} 
Rousseeuw, P. J. and van Driessen, K.
\textit{A fast algorithm for the minimum covariance determinant estimator.}
Technometrics 41, 212–223, 1999.
\end{thebibliography}


\clearpage
\appendix

\newgeometry{left=3cm, right=3cm}
\section{Code}
\FloatBarrier
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{myred}{rgb}{0.78,0,0.32}
\lstset{language=R,
        basicstyle=\scriptsize,
        breaklines=true,
        title=\lstname,
        frame=single,
        commentstyle=\color{mygreen},
        numberstyle=\tiny\color{mygray},
        keywordstyle=\color{blue},
        stringstyle=\color{myred}
}
\lstinputlisting{src/init.R}
\lstinputlisting{src/remove_outliers.R}
\lstinputlisting{src/outliers_plot.R}
\lstinputlisting{src/pairs.R}
\lstinputlisting{src/jitterboxplots.R}
\lstinputlisting{src/lm.R}

\clearpage
\FloatBarrier
\section{Extra figures}
\newgeometry{top=0.3cm}

\begin{figure}
  \widefig{img/jitterbox_engine_capacity}
  \widefig{img/jitterbox_urban_metric}
  \widefig{img/jitterbox_extra_urban_metric}
  \widefig{img/jitterbox_combined_metric}
  \widefig{img/jitterbox_noise_level}
  \widefig{img/jitterbox_co2}
  \widefig{img/jitterbox_co_emissions}
  \widefig{img/jitterbox_nox_emissions}
  \caption{Univariate distribution of each continuous variable, after the strongest outliers have been removed (as described in \sectionref{outlier_removal}). These are boxplots overlaid with point plots, where vertical jitter has been added to mitigate overplotting; this makes the distribution density more apparent.}
  \label{jitterboxes}
\end{figure}

\end{document}
