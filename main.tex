\documentclass[a4paper, 12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{float}
\usepackage{placeins}
\usepackage{xstring}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{color}
\usepackage{subcaption}
\usepackage[defaultlines=4,all]{nowidow}
\usepackage[hidelinks]{hyperref}
\usepackage{cleveref}


\geometry{tmargin=2.5cm, bmargin=3cm}

% Geen geïndenteerde paragrafen, da's maar lelijk.
\setlength{\parindent}{0pt}
% Wel spatie tussen paragrafen please.
\setlength{\parskip}{1em}

% Exponential notation in '\num{}'w/ Ex instead of *10^x
\sisetup{output-exponent-marker=\ensuremath{\mathrm{E}}}

% Necessary to make space->underscore substitution work. 
\usepackage[T1]{fontenc}
% Command that typesets variables in fixed width font, and with spaces automatically replaced by underscores.
% May need to be prepended by a '\protect' in a \caption.
\newcommand{\varr}[1]{\texttt{\StrSubstitute{#1}{ }{\_\allowbreak}}}

% For identifying individual datapoints -- i.e. cars.
\newcommand{\car}[3]{``#1 \textit{#2} #3''}

\newcommand{\wide}[1]{\makebox[\textwidth][c]{#1}%
}

% For figures accross the page -- wider than the text width
\newcommand{\widefig}[1]{\wide{\includegraphics[width=0.9\paperwidth]{#1}}}

\newcommand{\widesubfig}[2]{\wide{
  \centering
  \begin{subfigure}[b]{0.65\textwidth}
    #1
  \end{subfigure}
  \begin{subfigure}[b]{0.65\textwidth}
    #2
  \end{subfigure}
  }
}

\newcommand{\widetwofigs}[2]{\wide{
  \centering
  \begin{minipage}[t]{0.62\textwidth}
  #1
  \end{minipage}
  \hspace{1em}
  \begin{minipage}[t]{0.62\textwidth}
  #2
  \end{minipage}
 }
}

\newcommand{\sectionref}[1]{section ``\nameref{#1}''}

% Shortcut
\newcommand{\n}[1]{\num{#1}}

\begin{document}


\input{tex/titlepage}


\section{Linear regression}

The complete dataset of 1500 cars was randomly split into a 1000-car training set and a 500-car test set. We continue this analysis with the training set.

\subsection*{Exploratory analysis \& outlier removal}
\label{sec:RD_outlier_removal}

We search for outliers by calculating, for each data point, its statistical distance to the multivariate distribution -- using robust estimates for the mean and the covariance matrix. Specifically, the Minimum Covariance Determinant estimator \cite{mcd} was used, approximated by the \texttt{Fast MCD} algorithm \cite{fastmcd}. \Cref{fig:RD_outliers} shows the resulting robust distances.

\begin{figure}%[h]
  \widefig{fig/multivar_outlier_all}
  \vspace{1mm} \newline
  \widefig{fig/multivar_outlier_zoom}
  \caption{Robust distance-to-center of each car in the training set.}
  \label{fig:RD_outliers}
\end{figure}

Three strong outliers are found:
\begin{itemize}[topsep=0pt,itemsep=0pt]
  \item the \car{Volkswagen}{Jetta (from NOV 06 Wk 45 >)}{1.4 TSI (170 PS) Sport} with an abnormal \varr{noise level} of 0.3;
  \item the \car{Vauxhall}{Signum MY2008}{3.0CDTi V6 24v with 16/17/18" wheel} with an abnormal \varr{nox emissions} value of 237000;
  \item the \car{MG Rover Group}{Streetwise}{1.8} without any individually abnormal variables.
\end{itemize}
Note that this third outlier would not have been found had we only looked at univariate (or even bivariate) distributions of the data.

The found strong outliers are removed from the training set.


\begin{figure}%[H]
  \widefig{fig/pairs_euro-coloured}
  \caption{Uni- and bivariate distributions in the training set. The three strongest robust distance-outliers have been removed. Colours according to the future classification task. }
  \label{fig:pairs}
\end{figure}

\Cref{fig:pairs} shows the uni- and bivariate distributions in the data set. Note the very strong correlations between \varr{urban metric}, \varr{extra urban metric}, \varr{combined metric}, and \varr{co2} -- and to a lesser extend \varr{engine capacity}. This is in accordance with their meanings: cars with larger engine volumes consume more liters of fuel (all three \varr{ metrics} measure fuel consumption), and every liter of fuel corresponds to a fixed amount of $\mathrm{CO_2}$. These strong pairwise correlations point to the problem of multicollinearity in the data set. We explore this more formally further on.

We also note that for the variable \varr{noise level}, a large subset of cars take on discrete values. (There are also cars with \varr{noise level}s in between). This discrete character would negatively impact a cluster analysis, as the clusters would tend to form around the disrete values; while other variables would have disproportionately less impact on the clustering. However, we decide this will probably not impact the subsequent regression and classification tasks too much, so we keep this variable, for now.

Additionally, we note that none of the continuous variables seem to be univariate normally distributed (further on we perform numeric normality tests. Also normal QQ plots have been checked; they did not indicate any normality at all). Rather, their distributions are positively skewed, with heavy right tails.

Finally, note the multimodality of \varr{engine capacity} and \varr{nox emissions}, and the fact that \varr{fuel type} tends to split the other variables into groups -- see e.g. \varr{Diesel} versus \varr{Hybrid} or \varr{Petrol} for \varr{nox emissions}.


\subsection*{Model construction}

We will construct a general linear model to predict the \varr{co2} variable. The predictor variables to include in the model will be selected via bidirectional stepwise regression. The Akaike Information Criterion (AIC) is used to compare models. The least squares estimator will always be used to estimate the coefficients $\boldsymbol{\beta}$ for a candidate model. The categorical variables \varr{euro standard}, \varr{transmission type}, and \varr{fuel type} are expanded into indicator variables (e.g. \varr{fuel type Petrol}, taking on either a 0 or a 1).

\paragraph{Predictor variable transformations}
\label{sec:var_transform}\leavevmode\\
The AIC assumes the residuals to be normally distributed. As they are an affine transformation of the predictor variables, the predictor variables need to be normally distributed as well. Thus, before starting the stepwise variable selection procedure, we find a Box-Cox power transformation \cite{boxcox} for all continuous predictor variables that maximises their normality in the maximum likelihood sense. The thusly found exponents $\lambda$ are listed in \cref{table:predictorvar_transform}, along with the $p$-values for the Shapiro-Wilk test of normality before and after the transformation. Note that, except for \varr{noise level}, the normality of each variable greatly increases after the transformation. This was confirmed using normal QQ plots of the variables. We apply the maximum likelihood transformation to all continuous predictor variables, except for \varr{noise level} (as its maximum likelihood exponent falls outside the usual range, and its normality does not significantly improve after transformation).

\begin{table}
\centering
\input{tex/table_lambdas}
\caption{Maximum likelihood transformation exponents $\lambda$, and $p$-values for the Shapiro-Wilk test of normality before and after transformation, for each possible continuous predictor variable.}
\label{table:predictorvar_transform}
\end{table}

\paragraph{Response variable transformation}
\label{sec:responsevar_transfrom}\leavevmode\\
We now create a first linear model (via stepwise AIC regression starting from a model containing all predictor variables). \Cref{fig:pre-bc-response} shows the residuals of the resulting model. The strong (nonlinear) correlation between the predicted \varr{co2} values and the residuals prompts us to transform the response variable. The optimal Box-Cox power transformation of the response variable \cite{boxcox} is found for $\lambda = -0.3$ (see \cref{fig:lambda_ml}). We create a new linear model with the transformed response variable.

\begin{figure}
  \widetwofigs
  {
    \includegraphics[width=\textwidth, trim={0 3mm 0 2cm}, clip]
    {fig/pre-bc-response_pre-removal_fitted-stdres}
    \caption{Standardised residuals of the linear model versus fitted values, before the response variable has been transformed.}
    \label{fig:pre-bc-response}
  }{
    \includegraphics[width=\textwidth, trim={0 3mm 0 2cm}, clip]
    {fig/pre-bc-response_pre-removal_lambda-ml}
    \caption{Log-likelihood of $\lambda$ for the Box-Cox transformation of the response variable.}
    \label{fig:lambda_ml}
  }
\end{figure}

\paragraph{Removal of residual outliers}\leavevmode\\
The residuals of the model with transformed response variable are shown in \cref{fig:post-bc-response_pre-removal}. We find three strong vertical outliers. They are:
\begin{itemize}[topsep=0pt,itemsep=0pt]
  \item the \car{Volkswagen}{LT 35 Kombi MWB - LWB}{2.8 (158 PS) TDI Axle Ratio 3.727};
  \item the \car{Honda}{Insight, Model Year 2012}{1.3 IMA HS, HS-T, HX};
  \item and the \car{Renault}{New M\'egane Hatchback}{2.0 dCi 160 FAP}.
\end{itemize}
(These cars do not have higher robust distances than other cars (as in \cref{fig:RD_outliers}); they can however be spotted as outliers when the Mahalonobis distance is considered). These three additional outliers are removed from the training set. We note that removing these outliers has no effect on the maximum-likelihood $\lambda$ of the Box-Cox transformation of the response variable.

\begin{figure}
  \captionsetup[subfigure]{skip=0pt}
  \caption{Standardised residuals of the linear model versus fitted values, after the response variable has been transformed.}
  \widesubfig
  {
    \includegraphics[width=\textwidth, trim={0 3mm 0 2cm}, clip]
    {fig/post-bc-response_pre-removal_fitted-stdres}
    \caption{Before removal of vertical outliers.}
    \label{fig:post-bc-response_pre-removal}
  }{
    \includegraphics[width=\textwidth, trim={0 3mm 0 2cm}, clip]
    {fig/post-bc-response_post-removal_fitted-stdres}
    \caption{After removal of vertical outliers.}
    \label{fig:post-bc-response_post-removal}
  }
\end{figure}

\paragraph{Variable selection \& final model}\leavevmode\\
With optimal input and output transformations and the strongest outliers removed, we can now construct our final model, by selecting an appropriate subset of predictor variables. As already mentioned, bidirectional stepwise AIC regression is used. The procedure is started both from a model containing all predictor variables, and from an empty model containing only a constant term. Both procedures converge to the same model. The included variables, along with estimates for their coefficient and an analysis of variance (ANOVA), are shown in \cref{table:lm}. The variables that are not included, are \varr{noise level} and \varr{transmission type Manual}. We find a final $R^2$ value of 0.9987, an adjusted $R^2$ value of 0.9987, and a mean squared error $\hat{\sigma}^2$ of $4.750\times 10^{-7}$. The standardised residuals are shown in \cref{fig:post-bc-response_post-removal}.

\begin{table}
\centering
\hspace{-1cm}
\wide{\input{tex/table_lm}}
\caption{Coefficients and ANOVA of the final general linear model. $\lambda_j$'s taken from \cref{table:predictorvar_transform}. Note that both the predictor variables and the response variable are transformed non-linearly. That is why some coefficients may seem counterintuitive at first -- e.g. a negative slope for \protect\varr{engine capacity} with respect to \protect\varr{co2}.}
\label{table:lm}
\end{table}


\subsection*{Discussion}

\paragraph{Performance on the test set}\leavevmode\\
As we have a proper test set at our disposal, we can check whether our model generalises well. It does. The mean absolute relative error (defined as $\frac{1}{n-p}\sum_{i=1}^{n}{\left| \frac{\hat{y_i}-y_i}{y_i}\right|}$) on the test set is $2.9\times 10^{-3}$, demonstrating excellent predictive power -- the $\mathrm{CO_2}$ emissions of a car can be predicted with an error of on average only 0.3\%. The mean squared error on the test set is $8.00\times 10^{-7}$, indicating that our previous estimate $\hat{\sigma}^2 = 4.75\times 10^{-7}$ was a decent one.

\paragraph{Gauss-Markov conditions}\leavevmode\\
We check whether our final model satisfies the Gauss-Markov conditions. The first condition, that the errors have expectation zero, is satisfied by construction. The other two conditions -- that the errors have equal variance and are uncorrelated -- cannot be definitely proven, but we can check whether some necessary conditions seem to be satisfied. All residual plots -- residuals versus their index, residuals versus the fitted values, and residuals versus each of the independent variables -- need to be free of correlation. No curvature, funnel or trend should be visible. We have checked all possible residual plots -- both for the training set as well as for the test set. (One such plot can be seen in \cref{fig:post-bc-response_post-removal}). No visible correlations were found, supporting the hypothesis that the Gauss-Markov conditions are reasonably well satisfied.

\paragraph*{Normality}\leavevmode\\
Most steps in the construction and analysis of our model assume the errors to be normally distributed (e.g. the Box-Cox transformation of the response variable, the AIC, and the statistics in \cref{table:lm}). Based on the normal QQ plots of \cref{fig:qq_plots}, we pose that the errors are roughly normally distributed, but with tails that are too heavy. (The $p$ values for the Shapiro-Wilk test of normality are \n{0.01} and \n{<2E-16} for the training and test set, respectively). This less-than-ideal normality does not seem to be a problem during model construction. It does have the result however that the statistics and $p$-values in \cref{table:lm} need to be taken with a grain of salt.

\begin{figure}
  \caption{Normal QQ plots of linear model residuals}
  \widesubfig
  {
    \includegraphics[width=\textwidth, trim={0 0 0 2cm}, clip]
    {fig/post-bc-response_post-removal_qq}
    \caption{Training set standardised residuals}
  }{
    \includegraphics[width=\textwidth, trim={0 0 0 2cmm}, clip]
    {fig/test_set_qq}
    \caption{Test set residuals}
  }
  \label{fig:qq_plots}
\end{figure}

\paragraph*{Multicollinearity}\leavevmode\\
In the exploratory analysis, we noted the strong pairwise correlations between \varr{urban metric}, \varr{extra urban metric}, \varr{combined metric}, and \varr{co2}. This points to the problem of multicollinearity. Indeed, when we perform more formal tests, strong evidence is found for multicollinearity in the data (cutoff values from \cite{mia}): the variance inflation factors (VIF) of these mentioned variables are all larger than 100, and the mean VIF is 456, significantly larger than 1. The condition number $ \sqrt{\lambda_{max} / \lambda_{min}} = 129 $ is greater than 30 (where $\lambda$ are the eigenvalues of the corrrelation matrix of the data). In the preceding linear regression, we have ignored this multicollinearity. In the following exercise however, we will use a biased regression method with reduced variance to mitigate its effects.

\pagebreak
\section{Classification}
\FloatBarrier

For the classification task, the variable \varr{euro standard} is transformed into a binary factor. Former Euro Standards 3 and 4 are considered ``old'' (0), while former Euro Standards 5 and 6 are considered ``new'' (1). We will predict this binary variable by fitting a logistic model, and performing linear and quadratic determinant analyses. But before any model is fit, we turn our attention to the multicollinearity problem.

\subsection*{Decorrelating regressors via PCA}

Instead of the regular unbiased estimator for the linear model parameters, we will choose an estimator that trades off some newly present bias for a large reduction in model variance. We choose for principal component analysis (PCA); which in this context is also sometimes called principal component regression. Our task is then to choose a suitable number $k$ of principal components (PC) to retain.

\begin{figure}
  \widetwofigs
  {
    \includegraphics[width=\textwidth]{fig/screeplot}
    \caption{Eigenvalues of the correlation matrix of the training data, in descending order.}
    \label{fig:screeplot}
  }{
    \includegraphics[width=\textwidth]{fig/k_search}
    \caption{Prediction error rates on the test set after fitting a logistic model on the training set using only the first $k$ PC. }
    \label{fig:k_search}
  }
\end{figure}

For each possible number $k$ of largest eigenvalues (and thus number of PC) to retain, we fitted a full logistic model predicting \varr{euro model} using the transformed input variables and the remaining regressing factors. This model was applied to the (identically transformed) test data. The resulting prediction error rates are shown in \cref{fig:k_search}.

\subsection*{Logistic model}

\begin{table}
\centering
%\hspace{-1cm}
\wide{\input{tex/table_lrm}}
\caption{}
\label{table:lrm}
\end{table}

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{fig/logistic_fit}
\caption{}
\label{fig:logisticfit}
\end{figure}
\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{fig/deviance_residuals}
\caption{}
\label{fig:devianceresiduals}
\end{figure}

\subsection*{Linear \& quadratic discriminant analysis}


\FloatBarrier
\begin{thebibliography}{9}
\bibitem{mcd} 
Rousseeuw, P. J. and Leroy, A. M.
(1987)
\textit{Robust Regression and Outlier Detection}. 
Wiley, 1987

\bibitem{fastmcd} 
Rousseeuw, P. J. and van Driessen, K.
(1999)
\textit{A fast algorithm for the minimum covariance determinant estimator.}
Technometrics 41, 212–223.

\bibitem{boxcox}
Box, G. E. P. and Cox, D. R.
(1964)
\textit{An analysis of transformations (with discussion).} Journal of the Royal Statistical Society B, 26, 211–252.

\bibitem{mia}
Hubert, M
(2017)
\textit{Statistische modellen en data-analyse.}
Acco. Deel II, p. 155-156.
\end{thebibliography}


\clearpage
\appendix

\newgeometry{left=3cm, right=3cm}
\section{Code}
\FloatBarrier
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{myred}{rgb}{0.78,0,0.32}
\lstset{language=R,
        basicstyle=\scriptsize,
        breaklines=true,
        title=\lstname,
        frame=single,
        commentstyle=\color{mygreen},
        numberstyle=\tiny\color{mygray},
        keywordstyle=\color{blue},
        stringstyle=\color{myred}
}
\lstinputlisting{src/init.R}
\lstinputlisting{src/remove_RD_outliers.R}
\lstinputlisting{src/outliers_plot.R}
\lstinputlisting{src/pairs.R}
\lstinputlisting{src/transformer.R}
\lstinputlisting{src/lm.R}
\lstinputlisting{src/evaluate_lm.R}
\lstinputlisting{src/multicol.R}
\lstinputlisting{src/classification_init.R}
\lstinputlisting{src/PCA.R}
\lstinputlisting{src/log_res.R}
\lstinputlisting{src/LDA_QDA.R}

\end{document}
